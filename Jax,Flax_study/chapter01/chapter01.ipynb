{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **chapter01 Jax/Flax를 공부하기 전에**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Jax/Flax에 대한 소개와 예시**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX는 구글 딥마인드에서 사용하고 있는 고성능 딥러닝 프레임워크이며, 딥마인드뿐만 아니라 허깅 페이스에서도 JAX로 변환한 모델들을 발표하고 있다. 해당 모델들을 간략하게 소개하면서 PyTorch로 이루어진 모델과 비교했을 때 어떤 장점이 있는지 파악할 것이다.\n",
    "\n",
    "한편, JAX/FLAX를 본격적으로 공부하기 앞서 함수형 프로그래밍에 대해 이해하는 것이 필요하다.\n",
    "TensorFlow나 PyTorch와 다르게 JAX는 함수형 프로그래밍을 일부 차용했으므로 해당 개념이 익숙하지 않다면 다소 어려움이 있을 수 있다.\n",
    "이에 이번 장에서는 함수형 프로그래밍에 대해서도 다룰 것이다.\n",
    "그다음 Jax/Flax에서 자주 사용하는 파이썬의 표준 라이브러리와 Jax 설치 방법에 대해서도 알아보도록 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.1 Jax란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX(잭스)를 한마디로 표현하면 자동 미분과 XLA를 결합해서 사용하는 고성능 머신러닝 프레임워크이다.\n",
    "JAx의 자동 미분의 경우 파이썬 함수뿐만 아니라 NumPy 함수에서도 적용할 수 있다.\n",
    "또한 일반적인 반복문과 재귀에서도 사용 가능하며 2차 미분도 가능하다.\n",
    "사실 자동 미분의 경우 JAX 뿐만 아니라 PyTorch, TensorFlow 등 대부분의 프레임워크들이 지원하고 있는 항목이다.\n",
    "JAX의 가장 큰 장점은 XLA를 적용해서 사용할 수 있다는 점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLA(Accelerated Linear Algebra)는 GPU에서, 그리고 구글에서 만든 딥러닝 전용 하드웨어인 TPU에서 NumPy 프로그램을 컴파일하고 실행할 수 있게 만든다. JAX의 경우 XLA를 이용하기 위해서 JIT 컴파일을 진행해 파이썬 함수를 XLA에 최적화된 커널로 변환한다. XLA로 컴파일을 진행할 경우 파이토치가 채택한 동적 그래프 방식보다 훨씬 빠른 속도로 학습과 추론이 가능하다. 그 외에도 JAX는 함수 변환에도 능해서 자동 벡터화도 쉽게 할 수 있으며, 단일 프로그램 다중 데이터 병렬 프로그래밍을 위한 병렬 변환도 쉽게 가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다만 현재 JAX는 구글의 공식적인 제품으로 등록되어 있지 않고, 연구 프로젝트로 개발되고 있다.\n",
    "JAX 자체는 현재 구글 리서치(Google Research)에서 관리하고 있으며 지속적인 업데이트 또한 진행되고 있다.\n",
    "\n",
    "* flax: 0.8.5\n",
    "* jax: 0.4.30\n",
    "* optax: 0.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.2 Flax란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flax는 구글 브레인 팀에서 구글 리서치 JAX 팀과 협업하면서 만들어졌고 현재는 오픈소스 커뮤니티로 개발되고 있다.\n",
    "실제로 구글 내에서 만들어지는 프로덕트들은 Flax로 이루어져 있다.\n",
    "Flax는 JAX + Flexibility를 합쳐져서 만들었으며 엔지니어들이 JAX를 좀 더 쉽게 사용할 수 있게 만든 프레임워크이며, 다른 딥러닝 프레임워크들처럼 레이어 개념을 지원한다.\n",
    "그뿐만 아니라 평가 지표와 같은 유틸리티 또한 지원하고 있으며 빠른 설치 또한 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX는 실제 고성능을 낼 수 있는 특성들을 가졌다고 할 수 있으며, Flax는 Jax를 조금 더 쉽게 사용할 수 있게 만든 프레임워크이다. \n",
    "실제로 Jax 기반으로 딥러닝 모델을 만든다고 한다면 대부분의 경우 Flax를 함께 사용하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.3 JAX로 이루어진 기타 프레임워크들**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥마인드 또한 Jax를 활용한 프레임워크들을 개발하고 사용하고 있다.\n",
    "Flax처럼 Jax를 간편하게 사용할 수 있는 프레임워크로 Haiku가 있으며, 강화학습 버전으로 Jax를 쉽게 사용할 수 있게 만든 RLax도 있다.\n",
    "또한 이미지 처리용인 Pix, 그래프 신경망 버전인 Jraph 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것과 별개로 Flax와 범용적으로 많이 사용하는 프레임워크는 Optax이다.\n",
    "Optax는 Optimization + Jax의 합성어이며 최적화를 진행할 때 많이 사용한다.\n",
    "Flax의 경우 Optimizer 파트가 없는 대신 Optax를 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 최근 Jax 생태계에서는 Equinox(이퀴녹스)에 대한 언급이 늘어나고 있다.\n",
    "Flax가 일반적인 딥러닝 프레임워크 사용자들이 Jax를 더 쉽게 활용할 수 잇도록 돕는다면, Equinox는 pytree를 기반으로 Jax의 본질적인 특성을 최대한 살리는 데 집중한다는 의견이 있다.\n",
    "앞으로 Jax 생태계를 위해 Flax와 함께 Equinox의 발전도 중요한 요소가 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.4 Jax 프레임워크 사용 예시**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글에서 나온 대부분의 모델들은 Jax를 활용하고 있으며, 허깅 페이스의 경우 속도 향상을 위해 기존에 있던 모델들을 Jax로 변환하고 있다.\n",
    "구글에서 나온 모델 중에서 대표적인 사례로는 구글 리서치에서 관리하는 vision_transformer, maxtext, flan-T5 등이 있으며, 모델 변환의 대표적인 사례로는 Whisper-JAX, dalle-mini가 있다.\n",
    "그중에서 가장 많이 사용되고 있는 Whisper-JAx에 대해 잠시 살펴보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper는 OpenAI가 음성인식 및 음성 번역을 하기 위해 만든 오픈소스 모델이다.\n",
    "Whisper의 경우 PyTorch 1.1.0.1로 만들어졌으며 단순히 영어만 인식하는 것이 아니라 한국어, 일본어, 스페인어, 네팔어 등 다양한 언어를 지원한다.\n",
    "Whisper-JAX는 OpenAI가 PyTorch로 만든 모델을 허깅 페이스 연구원이 JAX로 변환해서 만든 모델이다.\n",
    "Whisper-JAX의 경우 Whisper를 그대로 계승했기 때문에 질적 성능은 동일하다.\n",
    "그러나 JAX로 만들어졌기 때문에 추론과 학습 속도 면에서는 Whisper보다 훨씬 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 함수형 프로그래밍에 대한 이해**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수형 프로그래밍을 처음 접한다면, 절차적 프로그래밍(Procedural Programming)과 비교해서 생각하면 이해하기가 더 쉽다.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
