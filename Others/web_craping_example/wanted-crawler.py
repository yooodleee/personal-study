import requests
from bs4 import BeautifulSoup
from webConnection import connectWebDriver, scrollPage

from contextlib import closing
from multiprocessing import Pool, Manager
from itertools import repeat

import re
import sys
from fileIO import openJsonFile, closeJsonFile, saveError
from dbIO import readDB, insertDB, insertJobGroups, insertRecruitInfoList, insertRecruitInfo

import nltk
from nltk.corpus import stopwords
from konlpy.tag import Okt
from ckonlpy.tag import Twitter, Postprocessor
from ckonlpy.utils import load_wordset, load_ngram


# nltk.download('punkt')
# nltk.download('stopwords')
okt = Okt()
twitter = Twitter()
stopwordsKR = load_wordset('cleansing_data/korean_stopwords.txt', encoding='ANSI')
customStopwordsEN = load_wordset('cleansing_data/english_stopwords.txt', encoding='ANSI')
stopwordsEN = customStopwordsEN.union(set(stopwords.words('english')))
ngrams = load_ngram('cleansing_data/korean_ngram.txt')
userdicts = load_wordset('cleansing_data/korean_user_dict.txt')
twitter.add_dictionary(list(userdicts), 'Noun', force=True)


# ëª¨ë“  ì§êµ° url ëª¨ìœ¼ê¸°
def getJobGroups():
    res = requests.get(
        'https://www.wanted.co.kr/wdlist/518?country=kr&job_sort=job.latest_order&years=-1&locations=all')
    html = res.text
    soup = BeautifulSoup(html, "html.parser")

    jobGroups = []
    for elements in soup.find("div", class_="_2h5Qtv_8mK2LOH-yR3FTRs").find_all("li"):
        href = elements.find("a")["href"]
        span = elements.find("span")
        jobGroup = {'jobGroup': span.get_text(), 'url': "https://www.wanted.co.kr" + href}
        jobGroups.append(jobGroup)
        print(jobGroup)

    # insertDB("jobGroup", jobGroups)
    insertJobGroups(jobGroups)
    return jobGroups


# íŠ¹ì • ì§êµ° ì±„ìš© ê³µê³  ê°€ì ¸ì˜¤ê¸° 
def getRecruitInfoList(urlDict, recruitInfos):
    print(urlDict['jobGroup'])
    driver = connectWebDriver(urlDict['url'])
    scrollPage(driver)
    allRecruitInfo = driver.find_elements_by_xpath('//div[@class="_3D4OeuZHyGXN7wwibRM5BJ"]/a')

    if allRecruitInfo:
        for recruitInfo in allRecruitInfo:
            jobGroup = urlDict['jobGroup']
            recruitInfoUrl = recruitInfo.get_attribute('href')
            recruitInfo = {'jobGroup': jobGroup, 'url': recruitInfoUrl}
            recruitInfos.append(recruitInfo)
            # with open('data/recruitInfoList.csv', 'a', encoding='utf-8-sig', newline='') as file:
            #     writer = csv.writer(file)
            #     writer.writerow([jobGroup, recruitInfoUrl])
    driver.quit()


# ì§êµ° ë³„ ì±„ìš©ê³µê³  url ëª¨ìœ¼ê¸° 
def scrapRecruitList(groups):
    # origin_headers = ['ì§êµ°', 'url']
    # with open('data/recruitInfoList.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(origin_headers)

    recruitInfosByGroup = manager.list()
    with closing(Pool(processes=5)) as pool:
        pool.starmap(getRecruitInfoList, zip(groups, repeat(recruitInfosByGroup)))

    insertRecruitInfoList("recruitInfos", recruitInfosByGroup)
    print('ì§êµ°ë³„ ì±„ìš©ê³µê³ ë¦¬ìŠ¤íŠ¸ url ì €ì¥ ì™„ë£Œ!')
    return recruitInfosByGroup


# ì±„ìš© ìƒì„¸ ì •ë³´ Element ê°€ì ¸ì˜¤ê¸° 
def getAllElement(driver, recruitInfoUrl):
    whereElement, tagElements, companyElement, detailElements, whereElement, deadlineElement, workAreaElement \
        = '', '', '', '', '', '', ''
    try:
        companyElement = driver.find_element_by_xpath('//section[@class="Bfoa2bzuGpxK9ieE1GxhW"]/div/h6/a')
    except Exception:
        saveError("elementError", recruitInfoUrl, 'warning: companyElement is null')

    try:
        detailElements = driver.find_elements_by_xpath('//section[@class="_1LnfhLPc7hiSZaaXxRv11H"]/p')
    except Exception:
        saveError("elementError", recruitInfoUrl, 'warning: detailElements is null')

    try:
        tagElements = driver.find_elements_by_xpath('//div[@class="ObubI7m2AFE5fxlR8Va9t"]/ul/li/a')
    except Exception:
        saveError("elementError", recruitInfoUrl, 'warning: tagElements is null')

    try:
        whereElement = driver.find_element_by_xpath(
            '/html/body/div[1]/div/div[3]/div[1]/div[1]/div/section[2]/div[1]/span')
    except Exception:
        saveError("elementError", recruitInfoUrl, 'warning: whereElement is null')

    try:
        workAreaElement = driver.find_element_by_xpath(
            '/html/body/div[1]/div/div[3]/div[1]/div[1]/div/div[2]/section[2]/div[2]/span[2]')
    except Exception:
        saveError("elementError", recruitInfoUrl, 'warning: workAreaElement is null')

    try:
        deadlineElement = driver.find_element_by_xpath(
            '/html/body/div[1]/div/div[3]/div[1]/div[1]/div[1]/div[2]/section[2]/div[1]/span[2]')
    except Exception:
        saveError("elementError", recruitInfoUrl, 'warning: deadlineElement is null')

    return [whereElement, tagElements, companyElement, detailElements, workAreaElement, deadlineElement]


# ì±„ìš© ìƒì„¸ ì •ë³´ Elementsì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° 
def getInfosByElements(elements):
    tagPattern = '[^0-9a-zA-Zã„±-í—%:.~\n]'
    detailPattern = '[^0-9a-zA-Zã„±-í—%:.~ #+\n]'
    region, _ = elements[0].text.split('\n.\n') if elements[0] else [None, None]
    tags = [re.sub(pattern=tagPattern, repl='', string=tagElement.text) for tagElement in elements[1]] if elements[
        1] else []
    company = elements[2].text if elements[2] else None
    workArea = elements[4].text if elements[4] else None
    deadline = elements[5].text if elements[5] else None

    details, detailsNouns = [], []
    if not elements[3]: details = []
    for detailElement in elements[3]:
        detail = re.sub(pattern=detailPattern, repl='', string=detailElement.text).strip()
        details.append(detail)

        englishTokens = nltk.word_tokenize(re.sub(f'[^a-zA-Z]', ' ', detailElement.text).strip())
        english = [token.lower() for token in englishTokens if token not in stopwordsEN]
        postprocessor = Postprocessor(twitter, passtags='Noun', ngrams=ngrams, stopwords=stopwordsKR)
        koreanWords = postprocessor.pos(detail)
        # koreanWords = okt.nouns(detail)
        print(koreanWords)

        korean = [word for word, _ in koreanWords if len(word) > 1 and word != 'ì•±']
        others = re.findall('[\d{2}]ë…„]', detailElement.text)
        temp = []
        temp.extend(korean)
        temp.extend(english)
        temp.extend(others)
        detailsNouns.append(temp)
    print(detailsNouns)

    return region, tags, company, details, deadline, workArea, detailsNouns


def createrecruitInfo(contents):
    # headers = ['id', 'ì§êµ°', 'ì§€ì—­', 'êµ­ê°€', 'íƒœê·¸', 'íšŒì‚¬ëª…', 'íšŒì‚¬ì†Œê°œ', 'ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'í˜œíƒ ë° ë³µì§€', 'ë§ˆê°ì¼', 'ê·¼ë¬´ì§€']
    headers = ['ì§êµ°', 'ì§€ì—­', 'íƒœê·¸', 'íšŒì‚¬ëª…', 'íšŒì‚¬ì†Œê°œ', 'ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'í˜œíƒ ë° ë³µì§€', 'ë§ˆê°ì¼', 'ê·¼ë¬´ì§€', 'íšŒì‚¬ì†Œê°œ_ëª…ì‚¬', 'ì£¼ìš”ì—…ë¬´_ëª…ì‚¬', 'ìê²©ìš”ê±´_ëª…ì‚¬', 'ìš°ëŒ€ì‚¬í•­_ëª…ì‚¬', 'í˜œíƒ ë° ë³µì§€_ëª…ì‚¬']
    recruitInfo = {header: value for header, value in zip(headers, contents)}
    # with open('data/RecruitInfoLog.json', 'a', encoding='UTF-8') as file:
    #     json.dump(recruitInfo, file, indent=4, ensure_ascii=False)
    #     file.write('\n,')
    return recruitInfo


# ì±„ìš© ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
def getRecruitInfo(recruitInfoUrl, allRecruitInfo):
    print(recruitInfoUrl)

    pattern = '[^0-9]'
    group = recruitInfoUrl['jobGroup']
    url = recruitInfoUrl['url']
    # group = recruitInfoUrl[0]
    # url = recruitInfoUrl[1].strip()
    recruitInfoId = re.sub(pattern=pattern, repl='', string=url)

    try:
        driver = connectWebDriver(url)
    except Exception as error:
        saveError("connectionError", recruitInfoUrl, error.args)
        return

    recruitInfoElements = getAllElement(driver, recruitInfoUrl)
    region, tags, company, details, deadline, workArea, detailsNouns = getInfosByElements(recruitInfoElements)

    contents = []
    contents.append(recruitInfoId)
    contents.append(group)
    contents.append(region)
    contents.append(company)
    contents.extend(details)        # [mainTask, qualification, preference, welfare]
    contents.append(deadline)
    contents.append(workArea)
    contents.extend(detailsNouns)   # [mainTaskNouns, qualificationNouns]

    recruitInfo = createrecruitInfo(contents)
    allRecruitInfo.append(recruitInfo)

    insertRecruitInfo("recruitInfo", recruitInfo)
    # REQUIRED = [recruitInfoId,  region, company, workArea]
    # with open('data/recruitInfo/origin.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(contents)
    # with open('data/recruitInfo/tag.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(REQUIRED+tags)
    # # íšŒì‚¬ì†Œê°œ
    # with open('data/recruitInfo/introduction.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(REQUIRED+detailsNouns[0])
    # # ì£¼ìš”ì—…ë¬´
    # with open('data/recruitInfo/main_task.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(REQUIRED+detailsNouns[1])
    # # ìê²©ìš”ê±´
    # with open('data/recruitInfo/requirement.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow([recruitInfoId]+detailsNouns[2])
    # # ìš°ëŒ€ì‚¬í•­
    # with open('data/recruitInfo/preference.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(REQUIRED+detailsNouns[3])
    # # ë³µì§€ ë° í˜œíƒ
    # with open('data/recruitInfo/benefit.csv', 'a', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(REQUIRED+detailsNouns[4])
    print('ì™„ë£Œ!')
    driver.quit()



def scrapRecruitInfo(recruitInfoURLs):
    # origin_headers = ['id', 'ì§êµ°', 'ì§€ì—­', 'íšŒì‚¬ëª…', 'íšŒì‚¬ì†Œê°œ', 'ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'í˜œíƒ ë° ë³µì§€', 'ë§ˆê°ì¼', 'ê·¼ë¬´ì§€']
    # with open('data/recruitInfo/origin.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(origin_headers)
    # with open('data/recruitInfo/tag.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(['id','íƒœê·¸'])
    # with open('data/recruitInfo/introduction.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(['id','íšŒì‚¬ì†Œê°œ'])
    # with open('data/recruitInfo/main_task.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(['id','ì£¼ìš”ì—…ë¬´'])
    # with open('data/recruitInfo/requirement.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(['id','ìê²©ìš”ê±´'])
    # with open('data/recruitInfo/preference.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(['id','ìš°ëŒ€ì‚¬í•­'])
    # with open('data/recruitInfo/benefit.csv', 'w', encoding='utf-8-sig', newline='') as file:
    #     writer = csv.writer(file)
    #     writer.writerow(['id','í˜œíƒ ë° ë³µì§€'])

    allRecruitInfo = manager.list()
    openJsonFile('data/logs/RecruitInfoError.json')
    with closing(Pool(processes=5)) as pool:
        pool.starmap(getRecruitInfo, zip(recruitInfoURLs, repeat(allRecruitInfo)))
    print('ì±„ìš©ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ëª¨ë‘ ì™„ë£Œ!')

    closeJsonFile('data/logs/RecruitInfoError.json')


def main():
    # ì§êµ° ì •ë³´ ìˆ˜ì§‘
    print('ğŸ“Œ ì§êµ° ìˆ˜ì§‘ ì¤‘...')
    jobGroups = getJobGroups()

    # ì§êµ°ë³„ ì±„ìš©ê³µê³  URL ìˆ˜ì§‘
    print('ğŸ“Œ ì±„ìš© ê³µê³  URL ìˆ˜ì§‘ ì¤‘...')
    insertRecruitInfoList(jobGroups)

    # DBì—ì„œ ì±„ìš©ê³µê³  URL ì½ê¸°
    print('ğŸ“Œ ì±„ìš©ê³µê³  URL DBì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°...')
    recruitInfosByGroup = readDB('recruitInfos')

    # ì±„ìš©ê³µê³  ìƒì„¸ ìˆ˜ì§‘
    print('ğŸ“Œ ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì¤‘...')
    scrapRecruitInfo(recruitInfosByGroup)

    print('âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!')


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸš« í”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    manager = Manager()
    # # ëª¨ë“  ì§êµ° 
    # getJobGroups()

    # # íŠ¹ì • ì§êµ° ì±„ìš© ê³µê³  
    # insertRecruitInfoList()
    
    # # ì±„ìš© ê³µê³  ìƒì„¸ ì •ë³´ 
    # insertRecruitInfo()

    # # print('---------ì±„ìš©ì§êµ°---------------------------')
    # # jobGroups = getJobGroups()
    # #
    # # print('---------ì±„ìš©ê³µê³ ë¦¬ìŠ¤íŠ¸----------------------')
    # # recruitInfosByGroup = scrapRecruitList(jobGroups)
    # #
    # # print('---------ì±„ìš©ê³µê³ ---------------------------')
    # # scrapRecruitInfo()

    # recruitInfosByGroup = readDB('recruitInfos')
    # # with open('data/recruitInfoList.csv', 'r', encoding='utf-8-sig', newline='') as file:
    # #     recruitInfosByGroup = [line.split(',') for line in file]
    # # print(recruitInfosByGroup)
    # # recruitInfosByGroup = [['í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì','https://www.wanted.co.kr/wd/42882']]
    # # recruitInfosByGroup =[{'jobGroup': 'í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì', 'url': 'https://www.wanted.co.kr/wd/43046'}]
    # scrapRecruitInfo(recruitInfosByGroup)