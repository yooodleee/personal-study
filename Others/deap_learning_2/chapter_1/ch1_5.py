#계산 고속화
'''
신경망의 학습과 추론에 드는 연산량은 상당하다.
신경망은 얼마나 빠르게 계산하는가가 매우 중요하다.
-> 신경망 고속화에 도움되는 '비트 정밀도'와 'GPU'에 대해 소개한다.
'''

#비트 정밀도
'''
넘파이의 부도오수점 수는 기본적으로 64비트 데이터 타입을 사용한다.
(환경, os나 파이썬/넘파이 버전에 따라 바뀔 순 있다.)
'''
import numpy as np
a=np.random.randn(3)
print(a.dtype)
#float64

'''
신경망의 추론과 학습은 32비트 부동소수점 수로도 문제없이(인식률을 거의 떨어뜨리는 일 없이)수행할\
수 았다.
32비트는 64비트의 절반이므로, 메모리 관점에서는 항상 32비트가 더 좋다고 말할 수 있다.
또, 신경망 계산 시 데이터를 전송하는 '버스 대역폭bus bandwidth'이 병목이 되는 경우가 종종 있다.
이런 경우에도 데이터 타입이 작은 게 유리하다.
-> 계산 속도 측면에서도 32비트 부동소수점 수가 일반적으로 더 빠르다.
'''
b=np.random.randn(3).astype(np.float32)
print(b.dtype)
#float32

c=np.random.randn(3).astype('f')
print(c.dtype)
#float32

'''
신경망 추론으로 한정하면, 16비트 부동소수점 수를 사용해도 인식률이 거의 떨어지지 않는다.
넘파이에도 16비트 부동소수점 수가 있으나, 일반적으로 CPU와 GPU는 연산 자체를 32비트로 수행한다.
-> 16비트 부동소수점 수로 변환하더라도 계산 자체는 32비트로 이뤄진다.

학습된 가중치를 (파일에) 저장할 때는 16비트 부동소수점 수가 여전히 유효하다.
가중치 데이터를 16비트로 저장하면 32비트를 쓸 때보다 절반의 용량만 사용할 수 있다.
-> 학습된 가중치를 저장하는 경우에는 16비트 부동소수점 수로 변환한다.
'''

#GPU(쿠파이)
'''
딥러닝의 계산은 대량의 곱하기 연산으로 구성된다.
이 연산 대부분은 병렬로 계산할 수 있는데, 이 점에서는 CPU보다 GPU가 유리하다.
대부분의 딥러닝 프레임워크는 GPU도 지원한다.

쿠파이는 파이썬 라이브러리를 사용할 수 있다.
쿠파이는 GPU를 이용해 병렬 계산을 수행해주는 라이브러리인데, 이것은 엔비디아의 GPU에서만 동작한다.
또한, CUDA라는 GPU 전용 범용 병렬 컴퓨팅.플랫폼을 설치해야 한다.

쿠파이를 사용하면 엔비디아 GPU를 사용해 간단하게 병렬 계산을 수행할 수 있다.
중요한 것은 쿠파이는 넘파이와 호환되는 API를 제공한다는 것이다.
'''
import cupy as cp
x=cp.arange(6).reshape(2,3).astype('f')
print(x)
#([[0., 1., 2.,],
#   3., 4., 5., ]), dtype=float32

x.sum(axis=1)
#([3., 12., ]), dtype=float32

'''
쿠파이 사용법은 기본적으로 넘파이와 같다.
사용법은 같지만 GPU를 사용해 계산한다.
-> 넘파이로 작성한 코드를 GPU 용으로 변경하기 아주 쉽다.
-> numpy를 cupy로 대체해주면 된다.
'''
import sys
sys.path.append('..')
from common import config
