#ReLU를 사용할 때의 가중치 초깃값

'''
Xavier 초깃값은 활성화 함수가 선형인 것을 전제로 이끈 결과이다.
sigmoid 함수와 tanh 함수는 좌우 대칭이라 중앙 부근이 선형인 함수로 볼 수 있다.
-> Xavier 초깃값이 적당하다.

반면 ReLU를 이용할 때는 특화된 초깃값을 이용할 것을 권장한다.
-> He 초깃값은 앞 계층의 노드가 n개일 때, 표준편차가 sqrt(2/n)인 정규분포를 사용한다.

ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 해석할 수 있다.
표준편차가 0.01인 정규분포(std=0.01), Xavier 초깃값, ReLU 전용 He 초깃값일 떄의 결과를 살펴보자.

std=0.01일 떄의 각 층의 활성화값들은 아주 작은 값들이다.
신경망에 아주 작은 데이터가 흐른다는 것은 역전파 때 가중치의 기울기 역시 작아진다는 뜻이다.
-> 실제로 학습이 거의 이뤄지지 않을 것이다.

Xavier 초깃값 결과를 보면 층이 깊어지면서 치우침이 조금씩 커진다.
실제로 층이 깊어지면 활성화값들의 치우침도 커지고, 학습할 때 '기울기 소실' 문제를 일으킨다.

He 초깃값은 모든 층에서 균일하게 분포되어 있다.
층이 깊어져도 분포가 균일하게 유지되기에 역전파 때도 적절한 값이 나올 것을 기대할 수 있다.
'''