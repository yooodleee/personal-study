#경사법(경사하강법)
'''
기계학습 대부분 문제는 학습 단계에서 최적의 매개변수를 찾아낸다.
신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다.
최적이란, 손실 함수가 최솟값이 될 때의 매개변수 값이다.

그러나 일반적인 문제의 손실 함수는 매우 복잡하다.
매개변수의 공간이 광대하여 어디가 최솟값이 되는지를 짐작할 수 없다.
이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 작은 값)을 찾으려는 것이 경사법이다.

각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것이다.
그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말 나아갈 방향인지는 보장할 수 없다.
실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다.

*안장점saddle point
함수가 극솟값, 최솟값, 안장점이 되는 장소에서는 기울기가 0이다.
극솟값은 국소적인 최솟값, 즉 한정된 범위에서의 최솟값인 점이다.
안장점은 어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점이다.

경사법은 기울기가 0인 장소를 찾지만 그것이 반드시 최솟값이라고는 할 수 없다.
(극솟값이나 안장점일 가능성이 있다.)
복잡하고 찌그러진 모양의 함수라면 (대부분) 평평한 곳으로 파고들면서\
고원plateau이라 하는, 학습이 진행되지 않는 정체기에 빠질 수 있다.

기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을\
줄일 수 있다.
그래서 최솟값이 되는 장소를 찾는 문제(아니면 가능한 한 작은 값이 되는 장소를 찾는 문제)\
에서는 기울기 정보를 단서로 나아갈 방향을 정해야 한다.

*경사법
현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다.
그 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다.
-> 함수의 값을 점차 줄이는 것이 경사법gradient method.
-> 기계학습을 최적화하는 데 쓰이는 방법.

경사법은 최솟값을 찾을 떄는 경사 하강법gradient descent method, 최댓값을 구할 때는\
경사 상승법gradient

x_0=x_0 - eta df/dx_0
x_1=x_1 - eta df/dx_1   #1회에 해당하는 갱신, 단계를 반복

eta-> 갱신하는 양-> 학습률learning rate
한 번의 학습으로 얼만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정함.

변수의 값을 갱신하는 단계를 여러 번 반복하면서 서서히 함수의 값을 줄인다.
변수의 수가 늘아더 같은 식(각 변수의 편미분 값)으로 갱신하게 된다.

또한 학습률 값은 0.01이나 0.001 등 미리 특정 값으로 정해두어야 한다.
일반적으로 이 값이 너무 크거나 작으면 '좋은 장소'를 찾아갈 수 없다.
-> 신경망 학습에서는 보통 이 학습률을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다.
'''
import numpy as np

def numerical_gradient(f,x):    #동작 방식은 변수가 하나일 때의 수치 미분과 거의 동일
    #함수의 인수인 f는 함수, x는 넘파이 배열이므로 x의 각 원소에 대해 수치 미분을 구한다.
    h=1e-4
    grad=np.zeros_like(x)   #x와 형상이 같은 배열 형성, 그 원소가 모두 0인 배열

    for idx in range(x.size):
        tmp_val=x[idx]

        #f(x+h) 계산  
        x[idx]=tmp_val+h
        fxh1=f(x)

        #f(x-h) 계산
        x[idx]=tmp_val-h
        fxh2=f(x)

        grad[idx]=(fxh1-fxh2)/(2*h)
        x[idx]=tmp_val  #값 복원
    
    return grad

def gradient_descent(f, init_x, lr=0.01, step_num=100):
    #f:최적화하려는 함수, init_x:초깃값, lr:학습률, step_num:경사법에 따른 반복 함수
    x=init_x

    for i in range(step_num):
        grad=numerical_gradient(f,x)    #함수의 기울기
        x-=lr*grad  #기울기*학습률을 갱신하는 처리를 step_num만큼 반복함.
    return x

#경사법으로 f(x_0, x_1)=x_0**2 + x_1**2의 최솟값을 구하시오.
def function_2(x):
    return x[0]**2+x[1]**2

init_x=np.array([-3.0, 4.0])    #초깃값(-3.0, 4.0)
z1=gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)
print(z1)
#[-6.11110793e-10  8.14814391e-10]


#학습률이 너무 큰 예:   lr=10.0     -> 큰 값으로 발산
init_x=np.array([-3.0, 4.0])
z2=gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)
print(z2)
#[-2.58983747e+13 -1.29524862e+12]

#학습률이 너무 작은 예: lr=1e-10    -> 거의 갱신되지 않은 채 끝남.
init_x=np.array([-3.0, 4.0])
z3=gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)
print(z3)
#[-2.99999994  3.99999992]
'''
학습률 같은 매개변수-> 하이퍼파라미터hyper parameter(초매개변수)
-> 가중치와 편향 같은 신경망의 매개변수와 성질이 다른 매개변수이다.

신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해 '자동'으로 획득되는 반면\
학습률 같은 하이퍼파라미터들은 사람이 직접 설정해야 하는 매개변수이다.
-> 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 과정을 거쳐야 한다.
'''