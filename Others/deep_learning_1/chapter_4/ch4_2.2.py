#교차 엔트로피 오차cross_entropy_error,CEE
'''
E=-sigma t_k*log y^k

y_k:신경망의 출력
t_k:정답 레이블, 정답에 해당하는 인덱스 원소만 1이고 그 외에는 0(원-핫 인코딩)
-> 실질적으로 정답일 때의 추정(t_k=1 일때의 y_k)의 자연로그를 계산하는 식이 된다.
-> 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.

*log x
x가 1일 때 y는 0이 되고, x가 0에 가까워질수록 y의 값은 점점 작아진다.
-> 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다.
-> 반대로 정답일 떄의 출력이 작아질수록 오차는 커진다.
'''
import numpy as np

def cross_entropy_error(y,t):
    delta=1e-7  #np.log() 함수에 0을 입력하면 -무한대를 뜻하는 -inf를 방지(delta)
    return -np.sum(t*np.log(y+delta))   

t=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]    #정답은 '2'
y=[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]  #정답일 때의 출력이 0.6(교차 엔트로피 오차는 약 0.51)
z1=cross_entropy_error(np.array(y), np.array(t))
print(z1)   #-log 0.6
#0.510825457099338

y=[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]  #정답(2)일 때의 출력이(더 낮은) 0.1인 경우(교차 엔트로피 오차는 약 2.3)
z2=cross_entropy_error(np.array(y), np.array(t))
print(z2)   #-log 0.1
#2.302584092994546

'''
결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단-> 오차제곱합의 판단과 일치함.
'''