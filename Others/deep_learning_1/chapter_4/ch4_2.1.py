#오차제곱합sum_of_squares_for_error,SSE
'''
E=(1/2)*sigma (y_k-t_k)^2

y_k: 신경망의 출력(신경망이 추정한 값)-> 소프트맥스 함수의 출력(확률로 해석)
t_k: 정답 레이블(정답을 가리키는 위치의 원소는 1, 그 외에는 0), 원-핫 인코딩
k: 데이터의 차원 수

오차제곱합은 각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차(y_k-t_k)를 제곱한 후, 그 총합을 구한다.
'''
import numpy as np

def sum_squares_error(y,t):
    return 0.5*np.sum((y-t)**2)

#정답은 2
t=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

#예1: '2'일 확률이 가장 높다고 추정함(0.6), 신경망의 출력도 '2'에서 가장 높은 경우
y=[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
z1=sum_squares_error(np.array(y), np.array(t))
print(z1)
#0.097500000000000031

#예2: '7'일 확률이 가장 높다고 추정함(0.6), 정답은 '2'이지만, 신경망의 출력은 '7'에서 가장 높다.
y=[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
z2=sum_squares_error(np.array(y), np.array(t))
print(z2)
#0.09750000000000003

'''
첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다.
-> 오차제곱합의 기준으로 첫 번째 추정 결과가(오차가 더 작으니) 정답에 더 가깝다고 판단할 수 있다.
'''