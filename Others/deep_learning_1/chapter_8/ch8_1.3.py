#깊게 하는 이유
'''
ILSVRC로 대표되는 대규모 이미지 인식 대회의 결과에서 파악할 수 있다.
상위를 차지한 기번 대부분은 딥러닝 기반이며, 그 경향은 신경망을 더 깊게 만드는 방향으로 가고 잇다.
층의 깊이에 비례해 정확도가 좋아지고 있다.

신경망의 매개변수가 줄어든다.
층을 깊게 한 신경망은 깊지 않은 경우보다 적은 매개변수로 같은 수준의 표현력을 달성할 수 있다.
합성곱 연산에서의 필터 크기에 주목해 생각해본다.

5x5 필터로 구성된 합성곱 계층을 생각해보자.
출력 데이터의 각 노드가 입력 데이터의 어느 영역으로부터 계산되었는가?
각각의 출력 노드는 입력 데이터의 5x5 크기 영역에서 계산된다.

이번엔 3x3의 합성곱 연산을 2회 반복하는 경우를 생각해보자.
출력 노드 하나는 중간 데이터의 3x3 영역에서 계산된다.
그럼 중간 데이터의 3x3 영역은 그 전 입력 데이터의 어느 영역에서 계산될까?
-> 5x5 크기의 영역에서 계산되어 나오는 것을 알 수 있다.
-> 출력 데이터는 입력 데이터의 5x5 영역을 보고 계산하게 된다.

5x5의 합성곱 연산 1회는 3x3의 합성곱 연산을 2회 수행하여 대체할 수 있다.
게다가 전자의 매개변수가 25개(5x5)인 반면, 후자는 총 18(2x3x3)이며,\
매개변수 수는 층을 반복할 수록 적어진다.
그리고 그 개수의 차이는 층이 깊어질수록 커진다.

3x3의 합성곱 연산을 3회 반복하면 매개변수는 모두 27개가 되지만, 같은 크기의 영역을\
1회의 합성곱 연ㅅ나으로 '보기' 위해서는 7x7 크기의 필터, 즉 매개변수 49개가 필요하다.

*작은 필터를 겹쳐 신경망을 깊게 할 때의 장점은 매개변수 수를 줄여 넓은 수용 영역receptive field\
을 소화할 수 있다는 데 있다.(수용 영역은 뉴런에 변화를 일으키는 국소적인 공간 영역이다.)
게다가 층을 거듭하면서 ReLU 등의 활성화 함수와 합성곱 계층 사이에 끼워서 신경망의 표현력이 개선된다.
-> 활성화 함수가 신경망에 '비선형'을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현할 수 있다.


학습의 효율성도 층을 깊게 하는 것의 이점이다.
층을 깊게 함으로써 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있다.
CNN의 합성곱 계층이 정보를 계층적으로 추출하고 있음을 알 수 있었다.
합성곱 계층에서는 에지 등의 단순한 패턴에 뉴런이 반응하고 층이 깊어지면서\
텍스처와 사물의 일부와 같이 점차 더 복잡한 것에 반응한다.

신경망을 깊게 하면 학습해야 할 문제를 계층적으로 분해할 수 있다.
각 층이 학습해야 할 문제를 더 단순한 문제로 대체할 수 있다.

층을 깊게 하면 정보를 계층적으로 전달할 수 있다.
각 층이 학습해야 할 문제를 풀기 쉬운 단순한 문제로 분해할 수 있어 효율적으로 학습할 수 있다.
'''
