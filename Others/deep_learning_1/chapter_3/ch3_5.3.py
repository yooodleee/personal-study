#소프트맥스 함수의 특징
'''
1) 소프트맥스 함수의 출력은 0~1.0 사이의 실수이다.

2) 소프트맥스 함수의 출력의 총합은 1이다.

-> 소프트맥스 함수의 출력을 '확률'로 해석할 수 있다.

y[0]의 확률은 0.018%(1.8%), y[1]의 확률은 0.245%(24.5%), y[2]의 확률은 0.737%(73.7%)
-> 소프트맥스 함수를 이용함으로써 문제를 확률적(통계적)으로 대응할 수 있다.

소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다.
-> 지수 함수 y=exp(x)가 단조 증가 함수이기 때문.

신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식한다.
소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않는다.
결과적으로 신경망을 분류할 때는 출력층의 소프트맥스 함수를 생략해도 된다.

현업에서도 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는 것이\
일반적이다.
'''
import numpy as np


def softmax(a):
    c=np.max(a)
    exp_a=np.exp(a-c) #오버플로우 대책
    sum_exp_a=np.sum(exp_a)
    y=exp_a/sum_exp_a

    return y

a=np.array([0.3, 2.9, 4.0])
y=softmax(a)    #소프트맥스의 출력은 0~1.0 사이의 실수.
print(y)
#[0.01821127 0.24519181 0.73659691]

k=np.sum(y) #소프트맥스 함수 출력의 총합은 1(중요).
print(k)
#1.0