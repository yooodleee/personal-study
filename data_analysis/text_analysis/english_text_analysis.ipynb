{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[영문 분석 + 워드클라우드]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **영문 문서 제목의 키워드 분석하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **분석 미리보기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 목표: 검색한 영문 학술 문서의 제목에서 빈도 분석을 수행하여 키워드를 도출한다.\n",
    "* 핵심 개념: 텍스트 분석, 전처리, 토큰화, 불용어, 어간 추출, 워드클라우드\n",
    "* 데이터 수집: 'Big data'로 검색한 영문 학술 문서 제목(학술연구정보서비스에서 수집)\n",
    "* 데이터 준비:\n",
    "    1. 데이터 조합: pandas.concat()\n",
    "    2. 데이터 정제: re 정규식, stopwords.words(\"english\")\n",
    "    3. 데이터 변환: word_tokenize(), lower(), WordNetLemmatize()\n",
    "* 데이터 탐색 및 모델링:\n",
    "    1. 단어 빈도 검색: Counter()\n",
    "    2. 단어 빈도 히스토그램: matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **핵심 개념 이해하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비정형 빅데이터 분석이 가능해지면서 자연어 처리(natural language processing)가 중요한 분석 대상으로 떠오르고 있다.\n",
    "자연어 처리란 음성 또는 텍스트 형태의 자연어에서 정보를 추출하여 분석하는 것이다.\n",
    "영문 텍스트에서 단어를 분석하여 키워드를 추출하고 시각화를 수행하는 이 프로젝트도 자연어 처리 방법 중 하나이다.\n",
    "\n",
    "이번 프로젝트에서 수행할 영문 텍스트 분석은 영문 데이터에서 분석할 특징(feature)을 선정한 후 컴퓨터가 처리할 수 있는 벡터(Vector) 형태로 변환하고\n",
    "분석 기법을 적용하여 필요한 정보를 추출하는 과정으로 수행한다.\n",
    "이때 단어 빈도 분석 결과는 히스토그램으로 확인하고 워드클라우드로 시각화하여 키워드를 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **텍스트 분석**\n",
    "\n",
    "텍스트 분석(text analysis)은 자연어 처리와 데이터 마이닝이 결합하여 발전된 분야로 비정형 텍스트 데이터에서 정보를 추출하여 분석하는 방법이다.\n",
    "텍스트 분석 기법은 단어에 대한 분석을 기본으로 허며, 텍스트 분류(text classification), 텍스트 군집화(text clustering), 감성 분석\n",
    "(sentiment analysis) 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **전처리**\n",
    "\n",
    "전처리(preprocessing)는 분석 작업의 정확도를 높이기 위해 분석에 사용할 데이터를 먼저 정리하고 변환하는 작업이다.\n",
    "전처리에서는 필요에 따라 다음과 같은 작업을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정체(classing): 불필요한 기호나 문자를 제거하는 작업으로 주로 정규식을 이용하여 수행한다.\n",
    "* 정규화(normalization): 정제와 같은 의미지만 형태가 다른 단어를 하나의 형태로 통합하는 작업으로 대/소 문자 통합, 유사 의미의 단어 통합 등이 있다.\n",
    "* 토큰화(tokenization): 데이터를 토큰으로 정한 기본 단위로 분리하는 작업이다. 문장을 기준으로 분리하는 문장 토큰화, 단어를 기준으로 분리하는 단어 토큰화 등이 있다.\n",
    "* 불용어(stopword) 제거: 의미가 있는 토큰을 선별하기 위해 조사, 관사, 접미사처럼 분석할 의미가 없는 토큰인 불용어(stopword)를 제거한다.\n",
    "* 어간 추출(semming): 단어에서 시제, 단/복수, 진행형 등을 나타내는 다양한 어간(stem)을 잘라내어 단어의 형태를 일반화한다.\n",
    "* 표제어 추출(lemmatization): 단어에서 시제, 단/복수, 진행형 등을 나타내는 다양한 표제어(lemma)를 추출하여 단어의 형태를 일반화한다. 품사를 지정하여 표제어를 추출하는 것이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **워드클라우드**\n",
    "\n",
    "워드클라우드(word cloud)는 텍스트 분석에서 많이 사용하는 시각화 기법이다.\n",
    "문서의 핵심 단어를 시각적으로 돋보이게 만들어 키워드를 직관적으로 알 수 있게 하는 것으로 출현 빈도가 높을수록 단어를 크게 나타낸다.\n",
    "방대한 양의 텍스트 정보를 다루는 빅데이터 분석에서 주요 단어를 시각화하기 위해 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **패키지 설치하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from wordcloud) (2.2.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from wordcloud) (11.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from wordcloud) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\skn08-312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import STOPWORDS, WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('path_to_your_nltk_data_folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas: 엑셀, CSV 등의 파일을 읽어서 데이터프레임에 저장하고 작업한 데이터를 데이터 프레임 형태로 구성해 엑셀, CSV 등의 파일에 저장하는 작업을 수행하는 모듈\n",
    "* glob: 경로와 이름을 지정하여 파일을 다루는 파일 처리 작업을 위한 모듈\n",
    "* re: 메타 문자를 이용하여 특정 규칙을 작성하는 정규식을 사용하기 위한 모듈\n",
    "* reduce: 2차원 리스트를 1차원 리스트로 차원을 줄이기 위한 모듈\n",
    "* word_tokenize: 자연어 처리 패키지(from nltk.tokenize) 중에서 단어 토큰화 작업을 위한 모듈\n",
    "* stopwords: 자연어 처리 패키지(from nltk.corpus) 중에서 불용어 정보를 제공하는 모듈\n",
    "* WordNetLemmatizer: 자연어 처리 패키지(from nltk.stem) 중에서 단어 형태의 일반화를 위해 표제어 추출을 제공하는 모듈\n",
    "* Counter: 데이터 집합에서 개수를 자동으로 계산하기 위한 모듈\n",
    "* matplotlib.pyplot: 히스토그램을 그리기 위한 matplotlib 패키지의 내부 모듈\n",
    "* STOPWORDS, WordCloud: 워드클라우드를 그리기 위해 사용할 워드클라우드용 불용어 모듈과 워드클라우드 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **데이터 조합 - 파일 병합하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1,000개의 데이터가 10개의 파일로 저장되어 있으므로 파일을 하나로 병합해야 한다.\n",
    "우선 병합할 엑셀 파일 이름 10개를 리스트에 저장한다.\n",
    "glob 함수를 사용하여 간단히 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('./myCabinetExcelData*.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\myCabinetExcelData (1).xls',\n",
       " '.\\\\myCabinetExcelData (2).xls',\n",
       " '.\\\\myCabinetExcelData (3).xls',\n",
       " '.\\\\myCabinetExcelData (4).xls',\n",
       " '.\\\\myCabinetExcelData (5).xls',\n",
       " '.\\\\myCabinetExcelData (6).xls',\n",
       " '.\\\\myCabinetExcelData (7).xls',\n",
       " '.\\\\myCabinetExcelData (8).xls',\n",
       " '.\\\\myCabinetExcelData (9).xls',\n",
       " '.\\\\myCabinetExcelData.xls']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일을 읽어서 하나의 데이터프레임으로 병합하고 CSV 파일에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data = []\n",
    "for file in all_files:\n",
    "    data_frame = pd.read_excel(file)\n",
    "    all_files_data.append(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>번호</th>\n",
       "      <th>제목</th>\n",
       "      <th>저자</th>\n",
       "      <th>출판사</th>\n",
       "      <th>출판일</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Guest Editorial: Big Media Data: Understanding...</td>\n",
       "      <td>Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C. C.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Guest Editorial: Big Scholar Data Discovery an...</td>\n",
       "      <td>Lin, Y.; Tong, H.; Tang, J.; Candan, K. S.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Guest Editorial: Big Data Analytics and the Web</td>\n",
       "      <td>Sheng, M.; Vasilakos, A. V.; Yu, Q.; You, L.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Parallel computing for preserving privacy usin...</td>\n",
       "      <td>Yaji, Sharath; Neelima, B.</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NoSQL databases for big data</td>\n",
       "      <td>Oussous, Ahmed; Benjelloun, Fatima-Zahra; Lahc...</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>Smart city big data analytics: An advanced review</td>\n",
       "      <td>Soomro, Kamran; Bhutta, Muhammad Nasir Mumtaz;...</td>\n",
       "      <td>John Wiley &amp; Sons Ltd</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>A watchdog approach - name-matching algorithm ...</td>\n",
       "      <td>Kirubakaran, Anusuya; Aramudhan, M.</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>An intelligent approach to Big Data analytics ...</td>\n",
       "      <td>Verma, Neha; Singh, Jatinder</td>\n",
       "      <td>Emerald Group Publishing Limited</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>How organisations leverage Big Data: a maturit...</td>\n",
       "      <td>Comuzzi, Marco; Patel, Anit</td>\n",
       "      <td>Emerald Group Publishing Limited</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>Effective and efficient distributed management...</td>\n",
       "      <td>Cuzzocrea, Alfredo; Grasso, Giorgio Mario; Nol...</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   번호                                                 제목  \\\n",
       "0          NaN    1  Guest Editorial: Big Media Data: Understanding...   \n",
       "1          NaN    2  Guest Editorial: Big Scholar Data Discovery an...   \n",
       "2          NaN    3    Guest Editorial: Big Data Analytics and the Web   \n",
       "3          NaN    4  Parallel computing for preserving privacy usin...   \n",
       "4          NaN    5                       NoSQL databases for big data   \n",
       "..         ...  ...                                                ...   \n",
       "95         NaN   96  Smart city big data analytics: An advanced review   \n",
       "96         NaN   97  A watchdog approach - name-matching algorithm ...   \n",
       "97         NaN   98  An intelligent approach to Big Data analytics ...   \n",
       "98         NaN   99  How organisations leverage Big Data: a maturit...   \n",
       "99         NaN  100  Effective and efficient distributed management...   \n",
       "\n",
       "                                                   저자  \\\n",
       "0         Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C. C.   \n",
       "1          Lin, Y.; Tong, H.; Tang, J.; Candan, K. S.   \n",
       "2        Sheng, M.; Vasilakos, A. V.; Yu, Q.; You, L.   \n",
       "3                          Yaji, Sharath; Neelima, B.   \n",
       "4   Oussous, Ahmed; Benjelloun, Fatima-Zahra; Lahc...   \n",
       "..                                                ...   \n",
       "95  Soomro, Kamran; Bhutta, Muhammad Nasir Mumtaz;...   \n",
       "96                Kirubakaran, Anusuya; Aramudhan, M.   \n",
       "97                       Verma, Neha; Singh, Jatinder   \n",
       "98                        Comuzzi, Marco; Patel, Anit   \n",
       "99  Cuzzocrea, Alfredo; Grasso, Giorgio Mario; Nol...   \n",
       "\n",
       "                                 출판사   출판일  \n",
       "0                            unknown  2015  \n",
       "1                            unknown  2016  \n",
       "2                            unknown  2016  \n",
       "3                       Inderscience  2018  \n",
       "4                       Inderscience  2017  \n",
       "..                               ...   ...  \n",
       "95             John Wiley & Sons Ltd  2019  \n",
       "96                      Inderscience  2018  \n",
       "97  Emerald Group Publishing Limited  2017  \n",
       "98  Emerald Group Publishing Limited  2016  \n",
       "99                      Inderscience  2019  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat = pd.concat(\n",
    "    all_files_data,\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>번호</th>\n",
       "      <th>제목</th>\n",
       "      <th>저자</th>\n",
       "      <th>출판사</th>\n",
       "      <th>출판일</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Guest Editorial: Big Media Data: Understanding...</td>\n",
       "      <td>Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C. C.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Guest Editorial: Big Scholar Data Discovery an...</td>\n",
       "      <td>Lin, Y.; Tong, H.; Tang, J.; Candan, K. S.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Guest Editorial: Big Data Analytics and the Web</td>\n",
       "      <td>Sheng, M.; Vasilakos, A. V.; Yu, Q.; You, L.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Parallel computing for preserving privacy usin...</td>\n",
       "      <td>Yaji, Sharath; Neelima, B.</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NoSQL databases for big data</td>\n",
       "      <td>Oussous, Ahmed; Benjelloun, Fatima-Zahra; Lahc...</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>Guest Editorial: Big Media Data: Understanding...</td>\n",
       "      <td>Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>Guest Editorial: Big Scholar Data Discovery an...</td>\n",
       "      <td>Lin, Y.; Tong, H.; Tang, J.; Candan, K. S.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>Guest Editorial: Big Media Data: Understanding...</td>\n",
       "      <td>Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C. C.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>Speed Up Big Data Analytics by Unveiling the S...</td>\n",
       "      <td>Wang, J.; Zhang, X.; Yin, J.; Wang, R.; Wu, H....</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>Architecting Time-Critical Big-Data Systems</td>\n",
       "      <td>Basanta-Val, P.; Audsley, N. C.; Wellings, A. ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   번호                                                 제목  \\\n",
       "0           NaN    1  Guest Editorial: Big Media Data: Understanding...   \n",
       "1           NaN    2  Guest Editorial: Big Scholar Data Discovery an...   \n",
       "2           NaN    3    Guest Editorial: Big Data Analytics and the Web   \n",
       "3           NaN    4  Parallel computing for preserving privacy usin...   \n",
       "4           NaN    5                       NoSQL databases for big data   \n",
       "..          ...  ...                                                ...   \n",
       "995         NaN   96  Guest Editorial: Big Media Data: Understanding...   \n",
       "996         NaN   97  Guest Editorial: Big Scholar Data Discovery an...   \n",
       "997         NaN   98  Guest Editorial: Big Media Data: Understanding...   \n",
       "998         NaN   99  Speed Up Big Data Analytics by Unveiling the S...   \n",
       "999         NaN  100        Architecting Time-Critical Big-Data Systems   \n",
       "\n",
       "                                                    저자           출판사   출판일  \n",
       "0          Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C. C.       unknown  2015  \n",
       "1           Lin, Y.; Tong, H.; Tang, J.; Candan, K. S.       unknown  2016  \n",
       "2         Sheng, M.; Vasilakos, A. V.; Yu, Q.; You, L.       unknown  2016  \n",
       "3                           Yaji, Sharath; Neelima, B.  Inderscience  2018  \n",
       "4    Oussous, Ahmed; Benjelloun, Fatima-Zahra; Lahc...  Inderscience  2017  \n",
       "..                                                 ...           ...   ...  \n",
       "995           Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C.       unknown  2017  \n",
       "996         Lin, Y.; Tong, H.; Tang, J.; Candan, K. S.       unknown  2017  \n",
       "997        Wang, J.; Qi, G.; Sebe, N.; Aggarwal, C. C.       unknown  2016  \n",
       "998  Wang, J.; Zhang, X.; Yin, J.; Wang, R.; Wu, H....       unknown  2018  \n",
       "999  Basanta-Val, P.; Audsley, N. C.; Wellings, A. ...       unknown  2016  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat.to_csv('./riss_bigdata.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수집한 데이터에서 제목을 추출하여 전처리를 수행한다.\n",
    "여기에서 수행할 전처리는 영어가 아닌 단어는 제거하고 소문자로 정규화하여 단어 토큰화를하는 작업이다.\n",
    "그리고 불용어는 제거하고 단어 형태를 일반화하기 위해 표제어 추출 작업을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_title = all_files_data_concat['제목']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Guest Editorial: Big Media Data: Understanding...\n",
       "1      Guest Editorial: Big Scholar Data Discovery an...\n",
       "2        Guest Editorial: Big Data Analytics and the Web\n",
       "3      Parallel computing for preserving privacy usin...\n",
       "4                           NoSQL databases for big data\n",
       "                             ...                        \n",
       "995    Guest Editorial: Big Media Data: Understanding...\n",
       "996    Guest Editorial: Big Scholar Data Discovery an...\n",
       "997    Guest Editorial: Big Media Data: Understanding...\n",
       "998    Speed Up Big Data Analytics by Unveiling the S...\n",
       "999          Architecting Time-Critical Big-Data Systems\n",
       "Name: 제목, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "words = []  \n",
    "# nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for title in all_title:\n",
    "    EnWords = re.sub(r\"[^a-zA-Z]+\", \" \", str(title))    \n",
    "    EnWordsToken = nlp(EnWords.lower())\n",
    "    EnWordsTokenStop = [w for w in EnWordsToken if w not in stopWords]\n",
    "    EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop]\n",
    "    words.append(EnWordsTokenStopLemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리가 끝난 words는 2차원 리스트이므로 reduce() 함수를 사용하여 1차원 리스트로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = list(reduce(lambda x, y: x+y, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 탐색 및 분석 모델 구축**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **데이터 탐색 - 단어 빈도 구하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 분석 모델을 적용할 것인지 알아보려면 전처리가 끝난 데이터의 내용과 상태를 탐색해봐야 한다.\n",
    "단어의 구성을 탐색하기 위해 빈도를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'big': 1409, 'editorial': 17, 'guest': 13, 'medium': 11})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter({\n",
    "    'guest': 13,\n",
    "    'editorial': 17,\n",
    "    'big': 1409,\n",
    "    'medium': 11,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "\n",
    "for tag, counts in count.most_common(50):\n",
    "    if(len(str(tag)) > 1):\n",
    "        word_count[tag] = counts\n",
    "        print(\"%s: %d\" % (tag, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **데이터 탐색 - 히스토그램 그리기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 단어 빈도에 대한 탐색을 시각적으로 쉽게 하기 위해 히스토그램을 그려본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(\n",
    "    word_count,\n",
    "    key=word_count.get(),\n",
    "    reverse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values = sorted(\n",
    "    word_count.values(),\n",
    "    reverse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    len(word_count), sorted_values, align='center'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(\n",
    "    range(len(word_count)), list(sorted_keys), rotation='85'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **결과 시각화**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Big data' 영어 학술 문서 제목의 키워드 분석을 위해 연도별 학술문서의 수를 추출하여 그래프로 시각화하고 키워드에 대한 워드클라우드를 만들어본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **그래프 그리기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연도별 학술문서 수를 추출하여 그래프를 그려본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat['data_count'] = 0\n",
    "summary_year = all_files_data_concat.groupby(\n",
    "    '출판일', as_index=False\n",
    ")['doc_count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.Figure(figsize=(12, 5))\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"doc-count\")\n",
    "plt.grid(True)\n",
    "plt.plot(\n",
    "    range(len(summary_year)), summary_year['doc_count']\n",
    ")\n",
    "plt.xticks(\n",
    "    range(len(summary_year)), [text for text in summary_year['출판일']]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **워드클라우드 그리기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 워드클라우드를 이용한 시각화를 해본다.\n",
    "워드클라우드는 단어와 빈도로 구성된 딕셔너리 객체를 입력으로 사용한다.\n",
    "데이터 탐색 단계에서 상위 50개로 작성해둔 word_count 딕셔너리가 있으므로 이를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(\n",
    "    background_color='ivory',\n",
    "    stopwords=stopwords,\n",
    "    width=800,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cloud = wc.generate_from_frequencies(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.0), np.float64(1.0), np.float64(0.0), np.float64(1.0))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn08-312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
